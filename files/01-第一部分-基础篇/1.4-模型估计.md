# 模型估计

* **概念**

    - 模型：

        首先我们需要清楚什么是模型，模型是一个数学概念，是我们对一类事物的数学表达，例如对于平面上一条直线，数学上的描述为y = a * x + b， 那么对于平面直线，我们就可以用一个向量[a, b]来表示，向量[a, b]就可以称为一个模型；

    - 观测值：

        实际情况中，我们都是需要通过一堆离散的点(x, y)，计算出向量[a,b]的值，这些离散的点集合就是观测值；

    - 残差：

        对于任意的观测值(x, y), 其到模型的距离可以表示为||a * x + b - y||


    - 最小样本集：

        每一个模型的估算所需要的观测值数量是不一样的，这个跟模型的未知数个数相关，例如平面直线模型[a, b]的未知数为2个，2个平面点就可以解求平面直线模型；

    - 多余观测：

        实际应用过程中，观测值的数量是远远超过最小样本集的，超过最小样本数量的观测值，称为多余观测值，很显然为了确保模型估计的鲁棒性，我们一般是需要多余观测值的；

* **方法**

    - **直接解**

        构造方程： A * X = L
        - 当观测值数量 < 最小样本数 ： 无解
        - 当观测值数量 = 最小样本数 ： 克拉默法则
        - 当观测值数量 > 最小样本数 ： 线性问题 ？最小二乘解 ：SVD矩阵分解，构造一个线性最小二乘问题， 迭代优化

    * **Ransac解**

        - 随机选取最少样本数量的观测值
        - 直接法求解模型
        - 统计所有样本的残差内点数量
        - 更新当前模型、残差、内点以及迭代终止条件
        - 满足迭代终止条件，跳出循环，否则重复上述步骤

    * **迭代优化解**

        一般非线性最小化的问题采取迭代求解，对于求解模型，我们首先知道一个初始解x(0),然后在其领域内迭代求解最优值；一般而言，模型f(x)在x(0)处的泰勒展开如下：

        - 一阶泰勒展开

            ![](/assets/模型估计-一阶泰勒公式-0.gif)

        - 二阶泰勒展开

            ![](/assets/模型估计-二阶泰勒公式-0.gif)

        - 雅各比矩阵(Jacob)

            对于多元函数，取其一阶偏导构成雅各比矩阵J

        - 海森矩阵（Hessian)

            对于多元函数，取其二阶偏导构成海森矩阵H

        1.梯度下降法:

        目标函数f(x)一阶连续可微，基本思想是，从当前点x(k)出发，取f(x)在点x(k)处梯度下降最快的方向作为搜索方向。

        ![](/assets/模型估计-一阶泰勒公式-1.gif)

        1）给定初始点x(0)和迭代终止条件，令k = 0;

        2） 计算x(k)处的梯度，当满足迭代终止条件，停止迭代，输出x(k),否则进行下一步；

        3）在梯度下降方向，进行一维搜索，转2）

        2.牛顿法

        目标函数f(x)二阶连续可微，且二阶导数总是正定：

        ![](/assets/模型估计-牛顿法-1.gif)

        1）给定初始点x(0)和迭代终止条件， 令 k = 0;

        2）计算x(k)处的梯度，当满足终止条件，停止迭代，否则进入下一步；

        3）更新： ![](/assets/模型估计-牛顿法-3.gif) 转2）

        3.高斯牛顿法

        高斯牛顿法，不是对目标函数进行优化，而是对误差函数进行最小二乘估计，使得残差最小：

        ![](/assets/模型估计-高斯牛顿法-1.gif)

        1）给定初始点x(0)和迭代终止条件， 令 k = 0；

        2）计算x(k)处的梯度，当满足条件，停止迭代，否则进入下一步；

        3）更新：![](/assets/模型估计-高斯牛顿法-3.gif) 转2）

        4.L-M

        1）给定初始点x(0)和初始残差rms(0)以及迭代终止条件， 令 k = 0, ![](/assets/模型估计-LM-1.gif)

        2） 计算雅各比矩阵J, 构造增量正规方程:

        ![](/assets/模型估计-LM-2.gif)

        3）求解方程，得到增量：

        3.1）当残差rms(k + 1) < rms(k), 沿着该方向做梯度下降搜索；否则，

        3.2）当残差rms(k + 1) >= rms(k), ,重新求解，转3.1）

 * **实际应用**

    - 定义最小样本数
    - 定义直接解算法
    - Ransac获取初始解，并获取内点，初始残差
    - 在内点集合内，进行迭代优化


<code>

    struct Line2DSolver
    {
        enum
        {
            MIN_SAMPLES = 2;
            MODEL_NUM  = 1
        };
        //a * x + b * y + c = 0
        typedef double[3] ModelT;
        typedef struct Point2D
        {
            double x;
            double y;
        } DataT;

        typedef std::vector<DataT> DataSetT;

        //直接解
        void Fit(
            MAGIC_PIXEL_IN DataSetT& samples,
            MAGIC_PIXEL_IO std::vector<ModelT>& models);

        //残差函数
        double Error(
            MAGIC_PIXEL_IN ModelT& model,
            MAGIC_PIXEL_IN DataT& data);

        //迭代优化解
        void Adjust(
            MAGIC_PIXEL_IN DataSetT& inliners,
            MAGIC_PIXEL_IO ModelT& best);
    }

    template<typename Solver>
    class Ransac
    {
        typedef Solver::ModelT ModelT;
        typedef Solver::DataT DataT;
        typedef Solver::DataSetT DataSetT;
        typedef std::size_t IndexT;

        void Solve(
            MAGIC_PIXEL_IN DataSetT& samples,
            MAGIC_PIXEL_IO ModelT& best,
            MAGIC_PIXEL_IO std::vector<IndexT>& inliers,
            MAGIC_PIXEL_IO std::vector<double>& rms
        );

        //直接解
        void Fit(
            MAGIC_PIXEL_IN std::vector<IndexT>& samples,
            MAGIC_PIXEL_IO std::vector<ModelT>& models);

        //残差函数
        double Error(
            MAGIC_PIXEL_IN ModelT& model,
            MAGIC_PIXEL_IN IndexT& index);

        double Errors(MAGIC_PIXEL_IN ModelT& model);

        //迭代优化解
        void Adjust(
            MAGIC_PIXEL_IN std::vector<IndexT>& inliners,
            MAGIC_PIXEL_IO ModelT& best);

        protected:
            DataSetT _samples;
    }
</code>